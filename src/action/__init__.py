# src/action.py

import os
import json
from openai import OpenAI
from dotenv import load_dotenv

# .env íŒŒì¼ ë¡œë“œ ë° OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” (reasoning.pyì™€ ë™ì¼í•˜ê²Œ ì„¤ì •)
load_dotenv()
api_key_from_env = os.environ.get("OPENAI_API_KEY")
if api_key_from_env:
    client = OpenAI(api_key=api_key_from_env)
else:
    print("ğŸš¨ [Action] ì˜¤ë¥˜: OPENAI_API_KEY í™˜ê²½ ë³€ìˆ˜ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    client = None

def generate_safety_guideline(analysis_result):
    """
    [ì‹¤ì œ Action Layer í•¨ìˆ˜]
    VLM ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìœ„í—˜ ë“±ê¸‰ì— ë”°ë¼ ì¡°ì¹˜ë¥¼ ì·¨í•˜ê³ ,
    HIGH ìœ„í—˜ ì‹œ LLMì„ í˜¸ì¶œí•˜ì—¬ ë‹¤êµ­ì–´ ì•ˆì „ ì§€ì¹¨ì„ ìƒì„±í•©ë‹ˆë‹¤.
    """
    if not client:
        print("ğŸš« [Action] OpenAI í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•„ LLM í˜¸ì¶œì„ ê±´ë„ˆ<0xEB><0><0xA4>ë‹ˆë‹¤.")
        return {"status": "error_client_not_initialized"}

    risk_level = analysis_result.get("risk_level", "Unknown")
    reason = analysis_result.get("reason", "ì œê³µëœ ì´ìœ  ì—†ìŒ.")

    print(f"ğŸ“¢ [Action] ìœ„í—˜ ë“±ê¸‰ '{risk_level}'ì— ë”°ë¥¸ ì¡°ì¹˜ ì‹¤í–‰...")

    if risk_level == "LOW":
        print("â¡ï¸ [Action] ìœ„í—˜ ë“±ê¸‰ LOW: ë¡œê·¸ ê¸°ë¡ë§Œ ìˆ˜í–‰í•©ë‹ˆë‹¤.")
        # ì—¬ê¸°ì— ë¡œê·¸ ê¸°ë¡ ë¡œì§ ì¶”ê°€ (ì˜ˆ: íŒŒì¼ ì €ì¥, DB ì €ì¥ ë“±)
        return {"status": "logged"}

    elif risk_level == "MED":
        print("âš ï¸ [Action] ìœ„í—˜ ë“±ê¸‰ MED: í™•ì¸ ì•Œë¦¼ í‘œì‹œ.")
        print(f"   - í™•ì¸ í•„ìš”: {reason}")
        # ì—¬ê¸°ì— í™•ì¸ ì•Œë¦¼ UI í‘œì‹œ ë˜ëŠ” ë©”ì‹œì§€ ì „ì†¡ ë¡œì§ ì¶”ê°€
        return {"status": "confirmation_requested"}

    elif risk_level == "HIGH":
        print("ğŸš¨ [Action] ìœ„í—˜ ë“±ê¸‰ HIGH: LLM í˜¸ì¶œí•˜ì—¬ ë‹¤êµ­ì–´ ì•ˆì „ ì§€ì¹¨ ìƒì„±...")

        # LLMì—ê²Œ ì „ë‹¬í•  í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        prompt_for_llm = f"""
        ë‹¤ìŒì€ ìŠ¤ë§ˆíŠ¸ ê³µì¥ì—ì„œ ê°ì§€ëœ ì‹¬ê°í•œ ì•ˆì „ ìœ„í—˜ ìƒí™©ì…ë‹ˆë‹¤:
        ìƒí™© ì„¤ëª…: {reason}

        ì´ ìƒí™©ì— ëŒ€í•œ êµ¬ì²´ì ì¸ í–‰ë™ ì§€ì¹¨ì„ ìƒì„±í•´ì£¼ì„¸ìš”.
        ì§€ì¹¨ì€ ë‹¤ìŒ ì–¸ì–´ë¡œ ê°ê° ì‘ì„±í•´ì•¼ í•©ë‹ˆë‹¤: í•œêµ­ì–´, ì˜ì–´, ë² íŠ¸ë‚¨ì–´.

        ê²°ê³¼ëŠ” ì•„ë˜ì™€ ê°™ì€ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
        {{
          "guideline_ko": "...",
          "guideline_en": "...",
          "guideline_vi": "..."
        }}
        """

        try:
            # LLM API í˜¸ì¶œ (í…ìŠ¤íŠ¸ ìƒì„± ëª¨ë¸ ì‚¬ìš©, ì˜ˆ: gpt-4o ë˜ëŠ” gpt-3.5-turbo)
            response = client.chat.completions.create(
                model="gpt-4o", # ë˜ëŠ” ë¹„ìš© íš¨ìœ¨ì ì¸ ëª¨ë¸ ì„ íƒ
                messages=[
                    {"role": "user", "content": prompt_for_llm}
                ],
                max_tokens=500, # ì¶©ë¶„í•œ ê¸¸ì´ì˜ ì§€ì¹¨ ìƒì„± í—ˆìš©
                response_format={"type": "json_object"} # JSON ì‘ë‹µ ê°•ì œ
            )

            guideline_str = response.choices[0].message.content
            guidelines = json.loads(guideline_str)

            print("--- [ìƒì„±ëœ ë‹¤êµ­ì–´ ì•ˆì „ ì§€ì¹¨] ---")
            print(f"ğŸ‡°ğŸ‡· (í•œêµ­ì–´): {guidelines.get('guideline_ko', 'ìƒì„± ì‹¤íŒ¨')}")
            print(f"ğŸ‡ºğŸ‡¸ (English): {guidelines.get('guideline_en', 'Generation failed')}")
            print(f"ğŸ‡»ğŸ‡³ (Tiáº¿ng Viá»‡t): {guidelines.get('guideline_vi', 'Táº¡o khÃ´ng thÃ nh cÃ´ng')}")
            print("---------------------------------")
            # ì—¬ê¸°ì— ìƒì„±ëœ ì§€ì¹¨ì„ ì‹¤ì œ ì‚¬ìš©ìì—ê²Œ ì „ë‹¬í•˜ëŠ” ë¡œì§ ì¶”ê°€ (ì˜ˆ: ì•± í‘¸ì‹œ, SMS ë“±)
            return {"status": "multilingual_guideline_generated", "guidelines": guidelines}

        except Exception as e:
            print(f"ğŸš¨ [Action] LLM API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return {"status": "error_llm_api_call"}

    else:
        print(f"â“ [Action] ì•Œ ìˆ˜ ì—†ëŠ” ìœ„í—˜ ë“±ê¸‰: {risk_level}")
        return {"status": "unknown_risk_level"}